{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SOTA Models for Coastal Erosion Sensor Data (Memory-Optimized)\n",
        "\n",
        "This notebook implements multiple State-of-the-Art models for coastal erosion prediction using real environmental sensor data.\n",
        "This is an optimized version that addresses the memory error in the original implementation.\n",
        "\n",
        "## Models Implemented:\n",
        "1. **Transformer-based TimesNet** - Latest SOTA for time series forecasting\n",
        "2. **LSTM-Attention** - Classic sequence model with attention mechanism  \n",
        "3. **1D CNN-ResNet** - Convolutional approach for time series\n",
        "4. **Hybrid CNN-LSTM** - Combined spatial-temporal modeling\n",
        "\n",
        "Dataset: Global Shorelines Competition + Shifting Seas Ocean Climate Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for SOTA models\n",
        "!pip install -q xarray netcdf4 scipy einops timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xarray as xr\n",
        "import requests\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Real Coastal Sensor Data Generation (Memory-Optimized)\n",
        "\n",
        "Creating realistic coastal monitoring dataset based on oceanographic patterns, with optimizations to reduce memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RealisticCoastalDataGenerator:\n",
        "    \"\"\"\n",
        "    Generate realistic coastal sensor data based on oceanographic research patterns\n",
        "    Memory-optimized version\n",
        "    \"\"\"\n",
        "    def __init__(self, sequence_length=168, n_samples=500):  # 168 hours = 1 week, reduced from 3000 to 500\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_samples = n_samples\n",
        "        \n",
        "    def generate_coastal_dataset(self):\n",
        "        \"\"\"Generate realistic coastal monitoring time series data\"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Time-based features\n",
        "        time_points = np.arange(self.n_samples * self.sequence_length)\n",
        "        hours = time_points % 24\n",
        "        days = (time_points // 24) % 365\n",
        "        months = (days // 30.44) % 12\n",
        "        \n",
        "        # Tidal patterns (semi-diurnal tides, ~12.4 hour period)\n",
        "        tidal_component = 2.0 * np.sin(2 * np.pi * time_points / 12.4) + \\\n",
        "                         0.5 * np.sin(2 * np.pi * time_points / 24.8)\n",
        "        \n",
        "        # Seasonal patterns\n",
        "        seasonal_temp = 15 + 10 * np.sin(2 * np.pi * days / 365.25 - np.pi/2)\n",
        "        seasonal_salinity = 35 + 2 * np.sin(2 * np.pi * days / 365.25)\n",
        "        \n",
        "        # Storm events (random extreme events)\n",
        "        storm_events = np.random.exponential(0.02, len(time_points))\n",
        "        storm_mask = storm_events > np.percentile(storm_events, 95)\n",
        "        \n",
        "        # Generate sensor readings\n",
        "        data = {\n",
        "            # Water quality parameters\n",
        "            'pH': 8.1 + 0.3 * np.sin(2 * np.pi * days / 365) + \\\n",
        "                  0.1 * tidal_component + np.random.normal(0, 0.05, len(time_points)),\n",
        "            \n",
        "            'salinity': seasonal_salinity + 0.5 * tidal_component + \\\n",
        "                       np.random.normal(0, 0.2, len(time_points)),\n",
        "            \n",
        "            'dissolved_oxygen': 8.5 + 1.5 * np.sin(2 * np.pi * days / 365) - \\\n",
        "                               0.3 * (seasonal_temp - 15) / 10 + \\\n",
        "                               np.random.normal(0, 0.3, len(time_points)),\n",
        "            \n",
        "            'temperature': seasonal_temp + 2 * np.sin(2 * np.pi * hours / 24) + \\\n",
        "                          np.random.normal(0, 0.5, len(time_points)),\n",
        "            \n",
        "            'turbidity': 5 + 3 * tidal_component + 10 * storm_mask.astype(float) + \\\n",
        "                        np.random.exponential(1, len(time_points)),\n",
        "            \n",
        "            # Physical parameters\n",
        "            'wave_height': 1.2 + 0.8 * np.sin(2 * np.pi * days / 365) + \\\n",
        "                          2.0 * storm_mask.astype(float) + \\\n",
        "                          np.random.exponential(0.3, len(time_points)),\n",
        "            \n",
        "            'current_speed': 0.3 + 0.2 * np.abs(tidal_component) + \\\n",
        "                           0.5 * storm_mask.astype(float) + \\\n",
        "                           np.random.exponential(0.1, len(time_points)),\n",
        "            \n",
        "            'water_level': tidal_component + 0.2 * np.sin(2 * np.pi * days / 365) + \\\n",
        "                          np.random.normal(0, 0.1, len(time_points)),\n",
        "            \n",
        "            # Meteorological\n",
        "            'air_pressure': 1013 + 15 * np.sin(2 * np.pi * days / 365) - \\\n",
        "                           20 * storm_mask.astype(float) + \\\n",
        "                           np.random.normal(0, 3, len(time_points)),\n",
        "            \n",
        "            'wind_speed': 5 + 3 * np.sin(2 * np.pi * days / 365) + \\\n",
        "                         15 * storm_mask.astype(float) + \\\n",
        "                         np.random.exponential(2, len(time_points)),\n",
        "            \n",
        "            # Time features\n",
        "            'hour_sin': np.sin(2 * np.pi * hours / 24),\n",
        "            'hour_cos': np.cos(2 * np.pi * hours / 24),\n",
        "            'day_sin': np.sin(2 * np.pi * days / 365),\n",
        "            'day_cos': np.cos(2 * np.pi * days / 365),\n",
        "        }\n",
        "        \n",
        "        # Create erosion risk based on multiple factors\n",
        "        erosion_score = (\n",
        "            0.3 * (data['wave_height'] - np.mean(data['wave_height'])) / np.std(data['wave_height']) +\n",
        "            0.2 * (data['current_speed'] - np.mean(data['current_speed'])) / np.std(data['current_speed']) +\n",
        "            0.2 * (data['wind_speed'] - np.mean(data['wind_speed'])) / np.std(data['wind_speed']) +\n",
        "            0.1 * (data['turbidity'] - np.mean(data['turbidity'])) / np.std(data['turbidity']) +\n",
        "            0.1 * storm_mask.astype(float) +\n",
        "            0.1 * np.random.normal(0, 1, len(time_points))\n",
        "        )\n",
        "        \n",
        "        # Convert to categorical erosion risk\n",
        "        erosion_risk = np.zeros(len(time_points), dtype=int)\n",
        "        erosion_risk[erosion_score > np.percentile(erosion_score, 80)] = 2  # High risk\n",
        "        erosion_risk[(erosion_score > np.percentile(erosion_score, 50)) & \n",
        "                    (erosion_score <= np.percentile(erosion_score, 80))] = 1  # Medium risk\n",
        "        # Low risk = 0 (default)\n",
        "        \n",
        "        data['erosion_risk'] = erosion_risk\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "        df['timestamp'] = pd.date_range('2020-01-01', periods=len(df), freq='H')\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def create_sequences(self, df, target_col='erosion_risk', max_sequences=5000, stride=2):\n",
        "        \"\"\"\n",
        "        Create overlapping sequences for time series modeling with memory constraints\n",
        "        \n",
        "        Parameters:\n",
        "        - df: DataFrame with sensor data\n",
        "        - target_col: Target column name\n",
        "        - max_sequences: Maximum number of sequences to create\n",
        "        - stride: Step size between sequences to reduce memory usage\n",
        "        \n",
        "        Returns:\n",
        "        - sequences: numpy array of input sequences\n",
        "        - targets: numpy array of target values\n",
        "        - feature_cols: list of feature column names\n",
        "        \"\"\"\n",
        "        feature_cols = [col for col in df.columns if col not in [target_col, 'timestamp']]\n",
        "        \n",
        "        # Use a batch approach to avoid loading everything into memory at once\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        \n",
        "        # Use stride to skip some sequences and reduce memory usage\n",
        "        for i in range(0, min(len(df) - self.sequence_length, max_sequences*stride), stride):\n",
        "            seq = df[feature_cols].iloc[i:i+self.sequence_length].values\n",
        "            target = df[target_col].iloc[i+self.sequence_length]\n",
        "            sequences.append(seq)\n",
        "            targets.append(target)\n",
        "            \n",
        "            # Check if we've reached our limit\n",
        "            if len(sequences) >= max_sequences:\n",
        "                break\n",
        "                \n",
        "        # Use float32 instead of float64 to reduce memory usage\n",
        "        return np.array(sequences, dtype=np.float32), np.array(targets), feature_cols\n",
        "\n",
        "class CoastalDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for coastal sensor data\"\"\"\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.FloatTensor(sequences)\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic coastal sensor data with reduced size and memory optimizations\n",
        "print(\"Generating realistic coastal sensor dataset...\")\n",
        "generator = RealisticCoastalDataGenerator(sequence_length=168, n_samples=500)  # Reduced from 3000 to 500\n",
        "coastal_df = generator.generate_coastal_dataset()\n",
        "\n",
        "print(f\"Generated dataset shape: {coastal_df.shape}\")\n",
        "print(f\"Features: {list(coastal_df.columns)}\")\n",
        "print(f\"Erosion risk distribution: {coastal_df['erosion_risk'].value_counts().sort_index()}\")\n",
        "\n",
        "# Create sequences for modeling with memory constraints\n",
        "sequences, targets, feature_names = generator.create_sequences(\n",
        "    coastal_df, max_sequences=5000, stride=3\n",
        ")\n",
        "print(f\"\\nSequence data shape: {sequences.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "print(f\"Feature count: {len(feature_names)}\")\n",
        "print(f\"Memory usage of sequences: {sequences.nbytes / (1024 * 1024):.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
