{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# GCL Dataset Machine Learning Training\n",
        "\n",
        "This notebook trains a machine learning model on the GCL (Global Coverage Layer) shapefile datasets from 2010, 2015, and 2020.\n",
        "\n",
        "## Dataset Overview\n",
        "The GCL dataset contains shapefiles with geographic data across multiple years:\n",
        "- GCL2010: Data from 2010\n",
        "- GCL2015: Data from 2015  \n",
        "- GCL2020: Data from 2020\n",
        "\n",
        "## Workflow\n",
        "1. **Print data structure** - Examine the shapefile structure and attributes\n",
        "2. **Prepare dataset** - Load and preprocess the shapefile data for training\n",
        "3. **Model architecture** - Define the ML model suitable for this data\n",
        "4. **Train model** - Train the model on the prepared dataset\n",
        "5. **Testing** - Evaluate model performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from shapely.geometry import Point, Polygon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Print Data Structure\n",
        "\n",
        "First, let's examine the structure of our GCL shapefiles to understand what attributes and data we're working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths to shapefiles\n",
        "shapefile_years = ['2010', '2015', '2020']\n",
        "shapefiles = {}\n",
        "\n",
        "# Load each shapefile and examine its structure\n",
        "for year in shapefile_years:\n",
        "    shapefile_path = f'GCL{year}.shp'\n",
        "    try:\n",
        "        # Load the shapefile\n",
        "        gdf = gpd.read_file(shapefile_path)\n",
        "        shapefiles[year] = gdf\n",
        "        \n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"GCL{year} Shapefile Structure:\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Shape: {gdf.shape}\")\n",
        "        print(f\"CRS: {gdf.crs}\")\n",
        "        print(f\"\\nColumns: {list(gdf.columns)}\")\n",
        "        print(f\"\\nData types:\")\n",
        "        print(gdf.dtypes)\n",
        "        print(f\"\\nFirst 5 rows:\")\n",
        "        print(gdf.head())\n",
        "        \n",
        "        # Print summary statistics for numeric columns\n",
        "        numeric_cols = gdf.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            print(f\"\\nSummary statistics:\")\n",
        "            print(gdf[numeric_cols].describe())\n",
        "        \n",
        "        # Print unique values for categorical columns (if any)\n",
        "        categorical_cols = gdf.select_dtypes(include=['object']).columns\n",
        "        categorical_cols = [col for col in categorical_cols if col != 'geometry']\n",
        "        if len(categorical_cols) > 0:\n",
        "            print(f\"\\nUnique values in categorical columns:\")\n",
        "            for col in categorical_cols:\n",
        "                unique_vals = gdf[col].nunique()\n",
        "                if unique_vals < 20:  # Only print if reasonable number of unique values\n",
        "                    print(f\"{col}: {gdf[col].unique()}\")\n",
        "                else:\n",
        "                    print(f\"{col}: {unique_vals} unique values\")\n",
        "                    \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GCL{year}.shp: {e}\")\n",
        "        \n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Data loading complete!\")\n",
        "print(f\"Successfully loaded {len(shapefiles)} shapefiles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Prepare Dataset for Training\n",
        "\n",
        "Now let's prepare the data for machine learning. We'll:\n",
        "- Extract features from the geometries\n",
        "- Combine data from different years \n",
        "- Create target variables based on temporal changes\n",
        "- Split into training and testing sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeospatialFeatureExtractor:\n",
        "    \"\"\"Extract features from geospatial data for ML training\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        \n",
        "    def extract_geometry_features(self, gdf):\n",
        "        \"\"\"Extract features from geometry objects\"\"\"\n",
        "        features = pd.DataFrame()\n",
        "        \n",
        "        # Basic geometry features\n",
        "        features['area'] = gdf.geometry.area\n",
        "        features['perimeter'] = gdf.geometry.length\n",
        "        features['centroid_x'] = gdf.geometry.centroid.x\n",
        "        features['centroid_y'] = gdf.geometry.centroid.y\n",
        "        \n",
        "        # Shape complexity metrics\n",
        "        features['shape_complexity'] = features['perimeter'] / (2 * np.sqrt(np.pi * features['area']))\n",
        "        features['area_perimeter_ratio'] = features['area'] / features['perimeter']\n",
        "        \n",
        "        # Bounding box features\n",
        "        bounds = gdf.geometry.bounds\n",
        "        features['bbox_width'] = bounds['maxx'] - bounds['minx']\n",
        "        features['bbox_height'] = bounds['maxy'] - bounds['miny']\n",
        "        features['bbox_area'] = features['bbox_width'] * features['bbox_height']\n",
        "        features['fill_ratio'] = features['area'] / features['bbox_area']\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def create_temporal_features(self, data_dict):\n",
        "        \"\"\"Create features based on temporal changes between years\"\"\"\n",
        "        years = sorted(data_dict.keys())\n",
        "        \n",
        "        if len(years) < 2:\n",
        "            print(\"Need at least 2 years of data for temporal features\")\n",
        "            return None\n",
        "            \n",
        "        # Extract features for each year\n",
        "        feature_dfs = {}\n",
        "        for year in years:\n",
        "            gdf = data_dict[year]\n",
        "            \n",
        "            # Extract geometry features\n",
        "            geom_features = self.extract_geometry_features(gdf)\n",
        "            \n",
        "            # Add year identifier\n",
        "            geom_features['year'] = int(year)\n",
        "            \n",
        "            # Add non-geometry attributes\n",
        "            non_geom_cols = [col for col in gdf.columns if col != 'geometry']\n",
        "            for col in non_geom_cols:\n",
        "                geom_features[f'attr_{col}'] = gdf[col]\n",
        "            \n",
        "            feature_dfs[year] = geom_features\n",
        "            \n",
        "        # Combine all years\n",
        "        combined_df = pd.concat(feature_dfs.values(), ignore_index=True)\n",
        "        \n",
        "        # Create change-based features (comparing consecutive years)\n",
        "        change_features = []\n",
        "        for i in range(1, len(years)):\n",
        "            prev_year = years[i-1]\n",
        "            curr_year = years[i]\n",
        "            \n",
        "            prev_df = feature_dfs[prev_year]\n",
        "            curr_df = feature_dfs[curr_year]\n",
        "            \n",
        "            # Calculate changes in numeric features\n",
        "            numeric_cols = ['area', 'perimeter', 'shape_complexity']\n",
        "            for col in numeric_cols:\n",
        "                if col in prev_df.columns and col in curr_df.columns:\n",
        "                    # Match geometries by index or location\n",
        "                    change = curr_df[col].values[:min(len(prev_df), len(curr_df))] - \\\n",
        "                            prev_df[col].values[:min(len(prev_df), len(curr_df))]\n",
        "                    \n",
        "                    change_df = pd.DataFrame({\n",
        "                        f'{col}_change': change,\n",
        "                        'year_transition': f'{prev_year}_{curr_year}'\n",
        "                    })\n",
        "                    change_features.append(change_df)\n",
        "        \n",
        "        return combined_df, change_features\n",
        "    \n",
        "    def prepare_ml_dataset(self, data_dict, task='classification'):\n",
        "        \"\"\"Prepare final dataset for machine learning\"\"\"\n",
        "        \n",
        "        # Create temporal features\n",
        "        combined_df, change_features = self.create_temporal_features(data_dict)\n",
        "        \n",
        "        if combined_df is None:\n",
        "            return None, None, None, None\n",
        "            \n",
        "        # Select numeric features for training\n",
        "        feature_cols = [col for col in combined_df.columns if col not in ['year'] and \n",
        "                       pd.api.types.is_numeric_dtype(combined_df[col])]\n",
        "        \n",
        "        X = combined_df[feature_cols]\n",
        "        \n",
        "        # Create target variable based on task\n",
        "        if task == 'classification':\n",
        "            # Example: Classify based on area quantiles\n",
        "            if 'area' in combined_df.columns:\n",
        "                y = pd.qcut(combined_df['area'], q=3, labels=['small', 'medium', 'large'])\n",
        "                y = self.label_encoder.fit_transform(y)\n",
        "            else:\n",
        "                # Create synthetic target if no obvious target exists\n",
        "                y = np.random.randint(0, 3, size=len(X))\n",
        "        else:\n",
        "            # Regression: predict area or another continuous variable\n",
        "            if 'area' in combined_df.columns:\n",
        "                y = combined_df['area']\n",
        "            else:\n",
        "                y = np.random.randn(len(X))\n",
        "        \n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.mean())\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        print(f\"Dataset prepared:\")\n",
        "        print(f\"  Total samples: {len(X)}\")\n",
        "        print(f\"  Features: {len(feature_cols)}\")\n",
        "        print(f\"  Training samples: {len(X_train)}\")\n",
        "        print(f\"  Testing samples: {len(X_test)}\")\n",
        "        print(f\"  Feature names: {feature_cols}\")\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test, feature_cols\n",
        "\n",
        "# Create feature extractor and prepare dataset\n",
        "extractor = GeospatialFeatureExtractor()\n",
        "\n",
        "# Only proceed if we have loaded shapefiles\n",
        "if len(shapefiles) > 0:\n",
        "    X_train, X_test, y_train, y_test, feature_names = extractor.prepare_ml_dataset(\n",
        "        shapefiles, task='classification'\n",
        "    )\n",
        "    \n",
        "    if X_train is not None:\n",
        "        print(f\"\\n✅ Dataset successfully prepared for training!\")\n",
        "        print(f\"Training set shape: {X_train.shape}\")\n",
        "        print(f\"Test set shape: {X_test.shape}\")\n",
        "        print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "    else:\n",
        "        print(\"⚠️ Could not prepare dataset. Check data structure.\")\n",
        "else:\n",
        "    print(\"⚠️ No shapefiles loaded. Cannot prepare dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "We'll implement both traditional ML models and a neural network for comparison. This allows flexibility in choosing the best approach for the geospatial data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch Dataset and DataLoader\n",
        "class GeospatialDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for geospatial features\"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Neural Network Architecture\n",
        "class GeospatialNN(nn.Module):\n",
        "    \"\"\"Neural network for geospatial classification\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], num_classes=3, dropout=0.2):\n",
        "        super(GeospatialNN, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        # Build hidden layers\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "            \n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Model factory for creating different architectures\n",
        "class ModelFactory:\n",
        "    \"\"\"Factory for creating different model types\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_random_forest(n_estimators=100):\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_gradient_boosting(n_estimators=100):\n",
        "        return GradientBoostingClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=5,\n",
        "            random_state=42\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_neural_network(input_dim, num_classes=3):\n",
        "        return GeospatialNN(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=[128, 64, 32],\n",
        "            num_classes=num_classes,\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "# Initialize models if data is available\n",
        "if 'X_train' in locals() and X_train is not None:\n",
        "    # Get dimensions\n",
        "    input_features = X_train.shape[1]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    \n",
        "    print(f\"Creating models for {input_features} features and {num_classes} classes...\")\n",
        "    \n",
        "    # Traditional ML models\n",
        "    rf_model = ModelFactory.create_random_forest()\n",
        "    gb_model = ModelFactory.create_gradient_boosting()\n",
        "    \n",
        "    # Neural network\n",
        "    nn_model = ModelFactory.create_neural_network(input_features, num_classes)\n",
        "    \n",
        "    print(\"✅ Models created successfully!\")\n",
        "    print(f\"  - Random Forest\")\n",
        "    print(f\"  - Gradient Boosting\")\n",
        "    print(f\"  - Neural Network: {sum(p.numel() for p in nn_model.parameters())} parameters\")\n",
        "else:\n",
        "    print(\"⚠️ No training data available. Load data first before creating models.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Train Models\n",
        "\n",
        "Let's train all three models and compare their performance. We'll track metrics and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utilities\n",
        "def train_neural_network(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
        "    \"\"\"Train neural network model\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                \n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += batch_y.size(0)\n",
        "                correct += (predicted == batch_y).sum().item()\n",
        "                \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        \n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "    \n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# Training execution\n",
        "if 'X_train' in locals() and X_train is not None:\n",
        "    print(\"Starting model training...\")\n",
        "    \n",
        "    # 1. Train Random Forest\n",
        "    print(\"\\n📊 Training Random Forest...\")\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "    print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "    \n",
        "    # 2. Train Gradient Boosting\n",
        "    print(\"\\n📊 Training Gradient Boosting...\")\n",
        "    gb_model.fit(X_train, y_train)\n",
        "    gb_pred = gb_model.predict(X_test)\n",
        "    gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "    print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
        "    \n",
        "    # 3. Train Neural Network\n",
        "    print(\"\\n📊 Training Neural Network...\")\n",
        "    \n",
        "    # Create DataLoaders\n",
        "    train_dataset = GeospatialDataset(X_train, y_train)\n",
        "    test_dataset = GeospatialDataset(X_test, y_test)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    # Train the model\n",
        "    train_losses, val_losses, val_accuracies = train_neural_network(\n",
        "        nn_model, train_loader, test_loader, epochs=50\n",
        "    )\n",
        "    \n",
        "    # Evaluate Neural Network\n",
        "    nn_model.eval()\n",
        "    nn_predictions = []\n",
        "    nn_true = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = nn_model(batch_X)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            nn_predictions.extend(predicted.cpu().numpy())\n",
        "            nn_true.extend(batch_y.numpy())\n",
        "    \n",
        "    nn_accuracy = accuracy_score(nn_true, nn_predictions)\n",
        "    print(f\"\\nNeural Network Final Accuracy: {nn_accuracy:.4f}\")\n",
        "    \n",
        "    # Summary of results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Random Forest:      {rf_accuracy:.4f}\")\n",
        "    print(f\"Gradient Boosting:  {gb_accuracy:.4f}\")\n",
        "    print(f\"Neural Network:     {nn_accuracy:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Store results\n",
        "    model_results = {\n",
        "        'Random Forest': {'accuracy': rf_accuracy, 'predictions': rf_pred},\n",
        "        'Gradient Boosting': {'accuracy': gb_accuracy, 'predictions': gb_pred},\n",
        "        'Neural Network': {'accuracy': nn_accuracy, 'predictions': nn_predictions}\n",
        "    }\n",
        "else:\n",
        "    print(\"⚠️ No training data available. Please load and prepare data first.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Testing and Evaluation\n",
        "\n",
        "Let's create comprehensive testing cells to evaluate model performance, visualize results, and analyze feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Confusion Matrices\n",
        "def plot_confusion_matrices(model_results, y_test):\n",
        "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    for idx, (model_name, results) in enumerate(model_results.items()):\n",
        "        cm = confusion_matrix(y_test, results['predictions'])\n",
        "        \n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
        "        axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.3f}')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute if models are trained\n",
        "if 'model_results' in locals():\n",
        "    plot_confusion_matrices(model_results, y_test)\n",
        "else:\n",
        "    print(\"⚠️ Train models first before testing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Feature Importance Analysis\n",
        "def plot_feature_importance(rf_model, feature_names, top_n=10):\n",
        "    \"\"\"Plot feature importance from Random Forest model\"\"\"\n",
        "    importances = rf_model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:top_n]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title(f'Top {top_n} Most Important Features')\n",
        "    plt.bar(range(top_n), importances[indices])\n",
        "    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
        "    plt.ylabel('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute if Random Forest is trained\n",
        "if 'rf_model' in locals() and 'feature_names' in locals():\n",
        "    plot_feature_importance(rf_model, feature_names)\n",
        "else:\n",
        "    print(\"⚠️ Train Random Forest model first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Training Curves for Neural Network\n",
        "def plot_training_curves(train_losses, val_losses, val_accuracies):\n",
        "    \"\"\"Plot training and validation curves\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Loss curves\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Accuracy curve\n",
        "    ax2.plot(epochs, val_accuracies, 'g-', label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute if neural network was trained\n",
        "if 'train_losses' in locals() and 'val_losses' in locals():\n",
        "    plot_training_curves(train_losses, val_losses, val_accuracies)\n",
        "else:\n",
        "    print(\"⚠️ Train neural network first to see training curves.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Detailed Classification Reports\n",
        "def print_classification_reports(model_results, y_test, class_names=None):\n",
        "    \"\"\"Print detailed classification reports for all models\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f'Class {i}' for i in range(len(np.unique(y_test)))]\n",
        "    \n",
        "    for model_name, results in model_results.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Classification Report: {model_name}\")\n",
        "        print('='*50)\n",
        "        print(classification_report(y_test, results['predictions'], \n",
        "                                  target_names=class_names))\n",
        "\n",
        "# Execute if models are trained\n",
        "if 'model_results' in locals():\n",
        "    # Define class names based on the classification task\n",
        "    class_names = ['Small', 'Medium', 'Large']  # Adjust based on your actual classes\n",
        "    print_classification_reports(model_results, y_test, class_names)\n",
        "else:\n",
        "    print(\"⚠️ Train models first before generating reports.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: Model Comparison Visualization\n",
        "def compare_models_barplot(model_results):\n",
        "    \"\"\"Create a bar plot comparing model accuracies\"\"\"\n",
        "    model_names = list(model_results.keys())\n",
        "    accuracies = [results['accuracy'] for results in model_results.values()]\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(model_names, accuracies)\n",
        "    \n",
        "    # Color bars based on performance\n",
        "    colors = ['green' if acc > 0.8 else 'orange' if acc > 0.6 else 'red' \n",
        "              for acc in accuracies]\n",
        "    for bar, color in zip(bars, colors):\n",
        "        bar.set_color(color)\n",
        "    \n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Performance Comparison')\n",
        "    plt.ylim(0, 1.0)\n",
        "    \n",
        "    # Add accuracy values on top of bars\n",
        "    for i, (name, acc) in enumerate(zip(model_names, accuracies)):\n",
        "        plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute if models are trained\n",
        "if 'model_results' in locals():\n",
        "    compare_models_barplot(model_results)\n",
        "else:\n",
        "    print(\"⚠️ Train models first before comparing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 6: Make Predictions on New Data\n",
        "def predict_new_samples(model, scaler, feature_names, n_samples=5):\n",
        "    \"\"\"Generate and predict on new synthetic samples\"\"\"\n",
        "    print(\"Generating predictions on new samples...\\n\")\n",
        "    \n",
        "    # Generate synthetic samples similar to the training data\n",
        "    new_samples = np.random.randn(n_samples, len(feature_names))\n",
        "    \n",
        "    # Scale the samples\n",
        "    new_samples_scaled = scaler.transform(new_samples)\n",
        "    \n",
        "    # Make predictions\n",
        "    if isinstance(model, nn.Module):\n",
        "        # Neural network prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = torch.FloatTensor(new_samples_scaled).to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            predictions = predictions.cpu().numpy()\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "    else:\n",
        "        # Scikit-learn model prediction\n",
        "        predictions = model.predict(new_samples_scaled)\n",
        "        probabilities = model.predict_proba(new_samples_scaled)\n",
        "    \n",
        "    # Display results\n",
        "    class_names = ['Small', 'Medium', 'Large']\n",
        "    for i in range(n_samples):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Predicted class: {class_names[predictions[i]]}\")\n",
        "        print(f\"  Class probabilities:\")\n",
        "        for j, class_name in enumerate(class_names):\n",
        "            print(f\"    {class_name}: {probabilities[i][j]:.3f}\")\n",
        "        print()\n",
        "\n",
        "# Execute prediction test\n",
        "if 'rf_model' in locals() and 'extractor' in locals():\n",
        "    print(\"Random Forest Predictions:\")\n",
        "    predict_new_samples(rf_model, extractor.scaler, feature_names, n_samples=3)\n",
        "    \n",
        "    if 'nn_model' in locals():\n",
        "        print(\"\\nNeural Network Predictions:\")\n",
        "        predict_new_samples(nn_model, extractor.scaler, feature_names, n_samples=3)\n",
        "else:\n",
        "    print(\"⚠️ Train models first before making predictions.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Save Trained Models\n",
        "\n",
        "Save the trained models for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "# Create models directory\n",
        "models_dir = Path('trained_models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save models if they are trained\n",
        "if 'rf_model' in locals():\n",
        "    # Save Random Forest\n",
        "    joblib.dump(rf_model, models_dir / 'random_forest_model.joblib')\n",
        "    print(\"✅ Random Forest model saved\")\n",
        "    \n",
        "if 'gb_model' in locals():\n",
        "    # Save Gradient Boosting\n",
        "    joblib.dump(gb_model, models_dir / 'gradient_boosting_model.joblib')\n",
        "    print(\"✅ Gradient Boosting model saved\")\n",
        "    \n",
        "if 'nn_model' in locals():\n",
        "    # Save Neural Network\n",
        "    torch.save({\n",
        "        'model_state_dict': nn_model.state_dict(),\n",
        "        'input_dim': nn_model.model[0].in_features,\n",
        "        'num_classes': nn_model.model[-1].out_features,\n",
        "    }, models_dir / 'neural_network_model.pt')\n",
        "    print(\"✅ Neural Network model saved\")\n",
        "    \n",
        "if 'extractor' in locals():\n",
        "    # Save the scaler and feature names\n",
        "    joblib.dump({\n",
        "        'scaler': extractor.scaler,\n",
        "        'label_encoder': extractor.label_encoder,\n",
        "        'feature_names': feature_names if 'feature_names' in locals() else None\n",
        "    }, models_dir / 'preprocessors.joblib')\n",
        "    print(\"✅ Preprocessors saved\")\n",
        "    \n",
        "print(f\"\\nAll models saved to: {models_dir.absolute()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
