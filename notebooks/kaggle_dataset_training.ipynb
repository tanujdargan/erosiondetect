{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SOTA Hybrid CNN-LSTM Training with Real Kaggle Datasets\n",
        "\n",
        "This notebook implements a State-of-the-Art Hybrid CNN-LSTM model for coastal erosion prediction using:\n",
        "1. **Global Shorelines Dataset** - NetCDF format from Kaggle Competition\n",
        "2. **Shifting Seas Ocean Climate and Marine Life Dataset** - Comprehensive oceanographic data\n",
        "\n",
        "## Model Architecture: Hybrid CNN-LSTM\n",
        "- **CNN**: Spatial feature extraction from multi-dimensional oceanographic data\n",
        "- **LSTM**: Temporal sequence modeling with bidirectional processing\n",
        "- **Attention**: Focus on critical time periods for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for real dataset processing\n",
        "!pip install -q xarray netcdf4 scipy einops kaggle\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "import xarray as xr\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "### Setup Kaggle API and Download Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup data directories\n",
        "data_dir = Path('../data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download datasets using Kaggle API\n",
        "# Note: You need to setup kaggle.json credentials first\n",
        "# Place your kaggle.json in ~/.kaggle/ or setup environment variables\n",
        "\n",
        "print(\"Setting up Kaggle datasets...\")\n",
        "print(\"Please ensure your Kaggle API credentials are configured.\")\n",
        "print(\"Instructions: https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
        "\n",
        "# Dataset URLs for reference:\n",
        "shoreline_dataset = \"globalshorelines/data\"\n",
        "marine_dataset = \"atharvasoundankar/shifting-seas-ocean-climate-and-marine-life-dataset\"\n",
        "\n",
        "print(f\"Dataset 1: {shoreline_dataset}\")\n",
        "print(f\"Dataset 2: {marine_dataset}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## SOTA Hybrid CNN-LSTM Model Implementation\n",
        "\n",
        "### Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridCNNLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    SOTA Hybrid CNN-LSTM model for coastal erosion prediction\n",
        "    Combines spatial feature extraction with temporal sequence modeling\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_classes, cnn_filters=[32, 64, 128], \n",
        "                 lstm_hidden=128, lstm_layers=2, dropout=0.1):\n",
        "        super(HybridCNNLSTM, self).__init__()\n",
        "        \n",
        "        # CNN feature extractor for spatial patterns\n",
        "        cnn_layers = []\n",
        "        in_channels = n_features\n",
        "        \n",
        "        for filters in cnn_filters:\n",
        "            cnn_layers.extend([\n",
        "                nn.Conv1d(in_channels, filters, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(filters),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.MaxPool1d(kernel_size=2, stride=1, padding=1)  # Mild downsampling\n",
        "            ])\n",
        "            in_channels = filters\n",
        "            \n",
        "        self.cnn_features = nn.Sequential(*cnn_layers)\n",
        "        \n",
        "        # Adaptive pooling to ensure consistent sequence length\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool1d(84)  # Reduce sequence length\n",
        "        \n",
        "        # Bidirectional LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_filters[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        \n",
        "        # Multi-head attention mechanism for LSTM outputs\n",
        "        lstm_output_dim = lstm_hidden * 2  # bidirectional\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_output_dim,\n",
        "            num_heads=8,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Layer normalization for attention\n",
        "        self.layer_norm = nn.LayerNorm(lstm_output_dim)\n",
        "        \n",
        "        # Advanced classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_output_dim // 2, lstm_output_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_output_dim // 4, n_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, n_features]\n",
        "        batch_size, seq_len, n_features = x.shape\n",
        "        \n",
        "        # Transpose for CNN: [batch_size, n_features, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        # CNN feature extraction\n",
        "        cnn_out = self.cnn_features(x)  # [batch_size, filters, seq_len']\n",
        "        \n",
        "        # Adaptive pooling to manage sequence length\n",
        "        cnn_out = self.adaptive_pool(cnn_out)  # [batch_size, filters, pooled_len]\n",
        "        \n",
        "        # Transpose back for LSTM: [batch_size, seq_len', filters]\n",
        "        cnn_out = cnn_out.transpose(1, 2)\n",
        "        \n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(cnn_out)  # [batch_size, seq_len', lstm_hidden*2]\n",
        "        \n",
        "        # Multi-head attention\n",
        "        attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        \n",
        "        # Residual connection and layer normalization\n",
        "        attended_out = self.layer_norm(attended_out + lstm_out)\n",
        "        \n",
        "        # Global average pooling over time dimension\n",
        "        pooled_output = attended_out.mean(dim=1)  # [batch_size, lstm_hidden*2]\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(pooled_output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"‚úÖ SOTA Hybrid CNN-LSTM model implemented\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Comprehensive Data Processing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KaggleDatasetProcessor:\n",
        "    \"\"\"\n",
        "    Unified processor for Global Shorelines and Shifting Seas datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, sequence_length=48):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.shoreline_data = None\n",
        "        self.marine_data = None\n",
        "        self.scaler = StandardScaler()\n",
        "        \n",
        "    def load_datasets(self):\n",
        "        \"\"\"Load both Kaggle datasets or create synthetic alternatives\"\"\"\n",
        "        print(\"üîÑ Loading datasets...\")\n",
        "        \n",
        "        # Try to load real datasets, fallback to synthetic\n",
        "        self.shoreline_data = self._load_shoreline_data()\n",
        "        self.marine_data = self._load_marine_data()\n",
        "        \n",
        "        print(\"‚úÖ Datasets loaded successfully\")\n",
        "        return self.shoreline_data, self.marine_data\n",
        "    \n",
        "    def _load_shoreline_data(self):\n",
        "        \"\"\"Load Global Shorelines NetCDF data\"\"\"\n",
        "        netcdf_files = list(self.data_dir.glob(\"*.nc\"))\n",
        "        \n",
        "        if not netcdf_files:\n",
        "            print(\"‚ö†Ô∏è No NetCDF files found. Creating synthetic shoreline data...\")\n",
        "            return self._create_synthetic_shoreline_data()\n",
        "        \n",
        "        try:\n",
        "            nc_file = netcdf_files[0]\n",
        "            print(f\"üìÅ Loading {nc_file.name}...\")\n",
        "            \n",
        "            data = xr.open_dataset(nc_file)\n",
        "            print(f\"‚úÖ Loaded shoreline data: {dict(data.dims)}\")\n",
        "            return data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading NetCDF: {e}\")\n",
        "            return self._create_synthetic_shoreline_data()\n",
        "    \n",
        "    def _create_synthetic_shoreline_data(self):\n",
        "        \"\"\"Create realistic synthetic shoreline data\"\"\"\n",
        "        print(\"üîß Creating synthetic shoreline data...\")\n",
        "        \n",
        "        # 14-year monthly data (2000-2013)\n",
        "        time_range = pd.date_range('2000-01-01', '2013-12-31', freq='M')\n",
        "        \n",
        "        # Global coastal coordinates\n",
        "        lat = np.linspace(-60, 70, 40)\n",
        "        lon = np.linspace(-180, 180, 80)\n",
        "        \n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Realistic shoreline change patterns\n",
        "        n_time, n_lat, n_lon = len(time_range), len(lat), len(lon)\n",
        "        \n",
        "        # Base shoreline position changes\n",
        "        shoreline_change = np.zeros((n_time, n_lat, n_lon))\n",
        "        \n",
        "        for i, timestamp in enumerate(time_range):\n",
        "            # Seasonal patterns\n",
        "            seasonal = 2 * np.sin(2 * np.pi * timestamp.month / 12)\n",
        "            \n",
        "            # Long-term sea level rise\n",
        "            trend = 0.15 * (timestamp.year - 2000)\n",
        "            \n",
        "            # Storm events (random)\n",
        "            storm_intensity = np.random.exponential(0.1, (n_lat, n_lon))\n",
        "            storm_mask = storm_intensity > np.percentile(storm_intensity, 95)\n",
        "            \n",
        "            # Combine effects\n",
        "            change = (seasonal + trend + \n",
        "                     5 * storm_mask * np.random.normal(1, 0.3, (n_lat, n_lon)) +\n",
        "                     np.random.normal(0, 1, (n_lat, n_lon)))\n",
        "            \n",
        "            shoreline_change[i] = change\n",
        "        \n",
        "        # Create comprehensive dataset\n",
        "        data = xr.Dataset({\n",
        "            'shoreline_position': (['time', 'lat', 'lon'], \n",
        "                                 np.cumsum(shoreline_change, axis=0)),\n",
        "            'erosion_rate': (['time', 'lat', 'lon'], \n",
        "                           np.gradient(shoreline_change, axis=0)),\n",
        "            'wave_energy': (['time', 'lat', 'lon'], \n",
        "                          np.abs(shoreline_change) * np.random.uniform(0.5, 2, shoreline_change.shape)),\n",
        "            'sea_level': (['time', 'lat', 'lon'],\n",
        "                        0.15 * np.arange(n_time).reshape(-1, 1, 1) + \n",
        "                        np.random.normal(0, 0.1, shoreline_change.shape))\n",
        "        }, coords={\n",
        "            'time': time_range,\n",
        "            'lat': lat,\n",
        "            'lon': lon\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Created synthetic shoreline data: {dict(data.dims)}\")\n",
        "        return data\n",
        "    \n",
        "    def _load_marine_data(self):\n",
        "        \"\"\"Load Shifting Seas marine dataset\"\"\"\n",
        "        csv_files = list(self.data_dir.glob(\"*.csv\"))\n",
        "        \n",
        "        if not csv_files:\n",
        "            print(\"‚ö†Ô∏è No CSV files found. Creating synthetic marine data...\")\n",
        "            return self._create_synthetic_marine_data()\n",
        "        \n",
        "        try:\n",
        "            # Look for marine/ocean related CSV files\n",
        "            marine_files = [f for f in csv_files if any(keyword in f.name.lower() \n",
        "                          for keyword in ['ocean', 'marine', 'sea', 'climate'])]\n",
        "            \n",
        "            if marine_files:\n",
        "                csv_file = marine_files[0]\n",
        "                print(f\"üìÅ Loading {csv_file.name}...\")\n",
        "                \n",
        "                data = pd.read_csv(csv_file)\n",
        "                print(f\"‚úÖ Loaded marine data: {data.shape}\")\n",
        "                return data\n",
        "            else:\n",
        "                return self._create_synthetic_marine_data()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading CSV: {e}\")\n",
        "            return self._create_synthetic_marine_data()\n",
        "    \n",
        "    def _create_synthetic_marine_data(self):\n",
        "        \"\"\"Create realistic synthetic marine/climate data\"\"\"\n",
        "        print(\"üîß Creating synthetic marine climate data...\")\n",
        "        \n",
        "        # 14 years of daily data\n",
        "        date_range = pd.date_range('2000-01-01', '2013-12-31', freq='D')\n",
        "        n_days = len(date_range)\n",
        "        \n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Create realistic oceanographic time series\n",
        "        data = pd.DataFrame({\n",
        "            'date': date_range,\n",
        "            'sea_surface_temp': 15 + 8 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + \n",
        "                               np.random.normal(0, 1, n_days),\n",
        "            'salinity': 35 + 2 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) + \n",
        "                       np.random.normal(0, 0.5, n_days),\n",
        "            'ph_level': 8.1 + 0.3 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                       np.random.normal(0, 0.1, n_days),\n",
        "            'dissolved_oxygen': 8 + 2 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                              np.random.normal(0, 0.5, n_days),\n",
        "            'wave_height': 1.5 + 0.8 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                          np.random.exponential(0.5, n_days),\n",
        "            'current_velocity': 0.3 + 0.2 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                              np.random.exponential(0.2, n_days),\n",
        "            'wind_speed': 8 + 5 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                         np.random.exponential(3, n_days),\n",
        "            'precipitation': np.random.exponential(2, n_days),\n",
        "            'atmospheric_pressure': 1013 + 10 * np.sin(2 * np.pi * np.arange(n_days) / 365.25) +\n",
        "                                  np.random.normal(0, 5, n_days)\n",
        "        })\n",
        "        \n",
        "        # Add derived features\n",
        "        data['year'] = data['date'].dt.year\n",
        "        data['month'] = data['date'].dt.month\n",
        "        data['day_of_year'] = data['date'].dt.dayofyear\n",
        "        \n",
        "        print(f\"‚úÖ Created synthetic marine data: {data.shape}\")\n",
        "        return data\n",
        "\n",
        "# Initialize processor\n",
        "processor = KaggleDatasetProcessor(data_dir, sequence_length=48)\n",
        "shoreline_data, marine_data = processor.load_datasets()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Pipeline and Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ErosionDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for erosion prediction\"\"\"\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = torch.FloatTensor(sequences)\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "def create_training_sequences(processor):\n",
        "    \"\"\"Create training sequences from both datasets\"\"\"\n",
        "    print(\"üîÑ Creating training sequences...\")\n",
        "    \n",
        "    # Process shoreline data for spatial-temporal patterns\n",
        "    shoreline_sequences = []\n",
        "    marine_sequences = []\n",
        "    all_targets = []\n",
        "    \n",
        "    # Extract shoreline sequences\n",
        "    if hasattr(processor.shoreline_data, 'erosion_rate'):\n",
        "        shore_data = processor.shoreline_data\n",
        "        time_len = len(shore_data.time)\n",
        "        \n",
        "        # Sample spatial locations\n",
        "        lat_samples = np.linspace(0, len(shore_data.lat)-1, 15, dtype=int)\n",
        "        lon_samples = np.linspace(0, len(shore_data.lon)-1, 30, dtype=int)\n",
        "        \n",
        "        for lat_idx in lat_samples[:5]:  # Reduced for memory\n",
        "            for lon_idx in lon_samples[:10]:\n",
        "                for start_idx in range(0, time_len - processor.sequence_length, 6):\n",
        "                    end_idx = start_idx + processor.sequence_length\n",
        "                    \n",
        "                    # Extract features at this location\n",
        "                    features = []\n",
        "                    for t in range(start_idx, end_idx):\n",
        "                        timestamp = shore_data.time.values[t]\n",
        "                        month = pd.to_datetime(timestamp).month\n",
        "                        \n",
        "                        feature_vector = [\n",
        "                            float(shore_data.shoreline_position[t, lat_idx, lon_idx]),\n",
        "                            float(shore_data.erosion_rate[t, lat_idx, lon_idx]),\n",
        "                            float(shore_data.wave_energy[t, lat_idx, lon_idx]),\n",
        "                            float(shore_data.sea_level[t, lat_idx, lon_idx]),\n",
        "                            lat_idx / len(shore_data.lat),  # Normalized coordinates\n",
        "                            lon_idx / len(shore_data.lon),\n",
        "                            np.sin(2 * np.pi * month / 12),\n",
        "                            np.cos(2 * np.pi * month / 12)\n",
        "                        ]\n",
        "                        features.append(feature_vector)\n",
        "                    \n",
        "                    shoreline_sequences.append(features)\n",
        "                    \n",
        "                    # Create target based on future erosion risk\n",
        "                    if end_idx < time_len - 3:\n",
        "                        future_erosion = float(shore_data.erosion_rate[end_idx:end_idx+3, lat_idx, lon_idx].mean())\n",
        "                        if future_erosion > 1.0:\n",
        "                            target = 2  # High risk\n",
        "                        elif future_erosion > 0.3:\n",
        "                            target = 1  # Medium risk\n",
        "                        else:\n",
        "                            target = 0  # Low risk\n",
        "                    else:\n",
        "                        target = 0\n",
        "                    \n",
        "                    all_targets.append(target)\n",
        "    \n",
        "    # Process marine/climate data\n",
        "    if isinstance(processor.marine_data, pd.DataFrame):\n",
        "        marine_df = processor.marine_data.copy()\n",
        "        \n",
        "        # Resample to monthly for consistency\n",
        "        marine_df['date'] = pd.to_datetime(marine_df['date'])\n",
        "        marine_monthly = marine_df.set_index('date').resample('M').mean()\n",
        "        \n",
        "        # Align with shoreline data timeframe\n",
        "        marine_monthly = marine_monthly.loc['2000-01':'2013-12']\n",
        "        \n",
        "        # Extract marine sequences\n",
        "        marine_features = ['sea_surface_temp', 'salinity', 'ph_level', 'dissolved_oxygen',\n",
        "                          'wave_height', 'current_velocity', 'wind_speed', 'precipitation',\n",
        "                          'atmospheric_pressure']\n",
        "        \n",
        "        for start_idx in range(0, len(marine_monthly) - processor.sequence_length, 3):\n",
        "            end_idx = start_idx + processor.sequence_length\n",
        "            \n",
        "            sequence_data = marine_monthly.iloc[start_idx:end_idx][marine_features].values\n",
        "            marine_sequences.append(sequence_data.tolist())\n",
        "    \n",
        "    # Combine sequences (pad marine sequences to match shoreline)\n",
        "    print(f\"Shoreline sequences: {len(shoreline_sequences)}\")\n",
        "    print(f\"Marine sequences: {len(marine_sequences)}\")\n",
        "    \n",
        "    # Use shoreline sequences as primary, pad with marine features if available\n",
        "    if shoreline_sequences:\n",
        "        sequences = np.array(shoreline_sequences, dtype=np.float32)\n",
        "        targets = np.array(all_targets, dtype=np.int64)\n",
        "        \n",
        "        # Add marine features if available\n",
        "        if marine_sequences and len(marine_sequences) > 0:\n",
        "            # Replicate marine patterns to match shoreline sequences\n",
        "            marine_array = np.array(marine_sequences[:len(sequences)], dtype=np.float32)\n",
        "            if marine_array.shape[0] == sequences.shape[0]:\n",
        "                # Combine features\n",
        "                combined_sequences = np.concatenate([sequences, marine_array], axis=-1)\n",
        "                sequences = combined_sequences\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(sequences)} sequences with shape {sequences.shape}\")\n",
        "    print(f\"Target distribution: {np.bincount(targets)}\")\n",
        "    \n",
        "    return sequences, targets\n",
        "\n",
        "# Create sequences\n",
        "sequences, targets = create_training_sequences(processor)\n",
        "\n",
        "# Data normalization\n",
        "scaler = StandardScaler()\n",
        "n_samples, seq_len, n_features = sequences.shape\n",
        "sequences_reshaped = sequences.reshape(-1, n_features)\n",
        "sequences_normalized = scaler.fit_transform(sequences_reshaped)\n",
        "sequences = sequences_normalized.reshape(n_samples, seq_len, n_features)\n",
        "\n",
        "# Create datasets\n",
        "dataset = ErosionDataset(sequences, targets)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"üìä Training set: {len(train_dataset)} samples\")\n",
        "print(f\"üìä Validation set: {len(val_dataset)} samples\")\n",
        "print(f\"üìä Features per timestep: {n_features}\")\n",
        "print(f\"üìä Sequence length: {seq_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_hybrid_model(model, train_loader, val_loader, epochs=50, lr=1e-3):\n",
        "    \"\"\"Train the Hybrid CNN-LSTM model with comprehensive evaluation\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', patience=7, factor=0.5, verbose=True\n",
        "    )\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'learning_rates': []\n",
        "    }\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 10\n",
        "    \n",
        "    print(\"üöÄ Starting Hybrid CNN-LSTM training...\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_samples = 0\n",
        "        \n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item() * data.size(0)\n",
        "            train_samples += data.size(0)\n",
        "            \n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}')\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_samples = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, targets in val_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                \n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "                \n",
        "                val_loss += loss.item() * data.size(0)\n",
        "                val_samples += data.size(0)\n",
        "                \n",
        "                # Predictions\n",
        "                pred = outputs.argmax(dim=1)\n",
        "                val_correct += pred.eq(targets).sum().item()\n",
        "                \n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_labels.extend(targets.cpu().numpy())\n",
        "        \n",
        "        # Calculate metrics\n",
        "        avg_train_loss = train_loss / train_samples\n",
        "        avg_val_loss = val_loss / val_samples\n",
        "        val_accuracy = val_correct / val_samples\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step(val_accuracy)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Store history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        \n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'  Val Accuracy: {val_accuracy:.4f}')\n",
        "        print(f'  Learning Rate: {current_lr:.6f}')\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_hybrid_cnn_lstm.pth')\n",
        "            print(f'  ‚úÖ New best validation accuracy: {best_val_acc:.4f}')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping after {epoch+1} epochs')\n",
        "            break\n",
        "        \n",
        "        print('-' * 60)\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_hybrid_cnn_lstm.pth'))\n",
        "    \n",
        "    # Final evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            pred = outputs.argmax(dim=1)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "    \n",
        "    # Generate comprehensive evaluation report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ FINAL MODEL EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"Final Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, \n",
        "                              target_names=['Low Risk', 'Medium Risk', 'High Risk']))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Low Risk', 'Medium Risk', 'High Risk'],\n",
        "                yticklabels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
        "    plt.title('Confusion Matrix - Hybrid CNN-LSTM')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "    \n",
        "    # Training curves\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    ax1.plot(history['train_loss'], label='Training Loss')\n",
        "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Loss Curves')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Accuracy curve\n",
        "    ax2.plot(history['val_accuracy'], label='Validation Accuracy', color='green')\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    # Learning rate\n",
        "    ax3.plot(history['learning_rates'], label='Learning Rate', color='red')\n",
        "    ax3.set_title('Learning Rate Schedule')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Learning Rate')\n",
        "    ax3.set_yscale('log')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "    \n",
        "    # Feature importance (attention visualization)\n",
        "    ax4.text(0.5, 0.5, f'Model Architecture:\\\\n\\\\n' + \n",
        "             f'‚Ä¢ CNN Filters: [32, 64, 128]\\\\n' +\n",
        "             f'‚Ä¢ LSTM Hidden: 128 (Bidirectional)\\\\n' +\n",
        "             f'‚Ä¢ Multi-head Attention: 8 heads\\\\n' +\n",
        "             f'‚Ä¢ Parameters: {sum(p.numel() for p in model.parameters()):,}\\\\n' +\n",
        "             f'‚Ä¢ Input Features: {n_features}\\\\n' +\n",
        "             f'‚Ä¢ Sequence Length: {seq_len}',\n",
        "             transform=ax4.transAxes, fontsize=12,\n",
        "             verticalalignment='center', horizontalalignment='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    ax4.set_title('Model Summary')\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "# Initialize and train model\n",
        "print(\"üèóÔ∏è Initializing SOTA Hybrid CNN-LSTM model...\")\n",
        "model = HybridCNNLSTM(\n",
        "    n_features=n_features,\n",
        "    n_classes=3,  # Low, Medium, High erosion risk\n",
        "    cnn_filters=[32, 64, 128],\n",
        "    lstm_hidden=128,\n",
        "    lstm_layers=2,\n",
        "    dropout=0.15\n",
        ")\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "# Train the model\n",
        "trained_model, training_history = train_hybrid_model(\n",
        "    model, train_loader, val_loader, epochs=50, lr=1e-3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Deployment and Usage\n",
        "\n",
        "### Key Features Implemented:\n",
        "\n",
        "1. **üåä Real Dataset Integration**: \n",
        "   - Global Shorelines NetCDF data (2000-2013)\n",
        "   - Shifting Seas Ocean Climate & Marine Life dataset\n",
        "   - Fallback to realistic synthetic data if downloads fail\n",
        "\n",
        "2. **üß† SOTA Hybrid CNN-LSTM Architecture**:\n",
        "   - 1D CNN for spatial feature extraction \n",
        "   - Bidirectional LSTM for temporal modeling\n",
        "   - Multi-head attention mechanism\n",
        "   - Advanced regularization and optimization\n",
        "\n",
        "3. **üìä Comprehensive Training Pipeline**:\n",
        "   - Data normalization and preprocessing\n",
        "   - Early stopping with learning rate scheduling\n",
        "   - Gradient clipping for stability\n",
        "   - Detailed evaluation metrics and visualizations\n",
        "\n",
        "4. **üéØ Erosion Risk Classification**:\n",
        "   - Low, Medium, High risk categories\n",
        "   - Based on future erosion rate predictions\n",
        "   - Combines spatial-temporal oceanographic patterns\n",
        "\n",
        "### Next Steps:\n",
        "- Download the actual Kaggle datasets for real-world performance\n",
        "- Experiment with different sequence lengths and model architectures\n",
        "- Add ensemble methods for improved robustness\n",
        "- Deploy model for real-time coastal monitoring\n",
        "\n",
        "### Usage Instructions:\n",
        "```python\n",
        "# To use the trained model for prediction:\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(new_sequences.to(device))\n",
        "    risk_levels = predictions.argmax(dim=1)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
